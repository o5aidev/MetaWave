import Foundation
import SoundAnalysis
import AVFoundation

// MARK: - éŸ³å£°æ„Ÿæƒ…åˆ†æçµæœ
struct VoiceEmotionResult {
    let emotion: EmotionType
    let confidence: Float
    let arousal: Float // èˆˆå¥®åº¦ (0.0 - 1.0)
    let valence: Float // ä¾¡å€¤åˆ¤æ–­ (0.0 - 1.0)
    let pitch: Float // ãƒ”ãƒƒãƒ (Hz)
    let volume: Float // éŸ³é‡ (dB)
    let speakingRate: Float // è©±é€Ÿ (words per minute)
    let timestamp: Date
}

// MARK: - æ„Ÿæƒ…ã‚¿ã‚¤ãƒ—
enum EmotionType: String, CaseIterable {
    case positive = "positive"
    case negative = "negative"
    case neutral = "neutral"
    case excited = "excited"
    case calm = "calm"
    case stressed = "stressed"
    
    var displayName: String {
        switch self {
        case .positive: return "ãƒã‚¸ãƒ†ã‚£ãƒ–"
        case .negative: return "ãƒã‚¬ãƒ†ã‚£ãƒ–"
        case .neutral: return "ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«"
        case .excited: return "èˆˆå¥®"
        case .calm: return "è½ã¡ç€ã"
        case .stressed: return "ã‚¹ãƒˆãƒ¬ã‚¹"
        }
    }
    
    var emoji: String {
        switch self {
        case .positive: return "ğŸ˜Š"
        case .negative: return "ğŸ˜”"
        case .neutral: return "ğŸ˜"
        case .excited: return "ğŸ¤©"
        case .calm: return "ğŸ˜Œ"
        case .stressed: return "ğŸ˜°"
        }
    }
}

// MARK: - éŸ³å£°æ„Ÿæƒ…åˆ†æã‚¨ãƒ©ãƒ¼
enum VoiceEmotionAnalysisError: Error, LocalizedError {
    case audioDataInvalid
    case analysisFailed
    case featureExtractionFailed
    case modelNotAvailable
    
    var errorDescription: String? {
        switch self {
        case .audioDataInvalid:
            return "éŸ³å£°ãƒ‡ãƒ¼ã‚¿ãŒç„¡åŠ¹ã§ã™"
        case .analysisFailed:
            return "éŸ³å£°æ„Ÿæƒ…åˆ†æã«å¤±æ•—ã—ã¾ã—ãŸ"
        case .featureExtractionFailed:
            return "éŸ³å£°ç‰¹å¾´é‡ã®æŠ½å‡ºã«å¤±æ•—ã—ã¾ã—ãŸ"
        case .modelNotAvailable:
            return "éŸ³å£°åˆ†æãƒ¢ãƒ‡ãƒ«ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“"
        }
    }
}

// MARK: - éŸ³å£°æ„Ÿæƒ…åˆ†æãƒ—ãƒ­ãƒˆã‚³ãƒ«
protocol VoiceEmotionAnalyzerProtocol {
    func analyzeEmotion(from audioData: Data) async throws -> VoiceEmotionResult
    func extractAudioFeatures(from audioData: Data) async throws -> AudioFeatures
}

// MARK: - éŸ³å£°ç‰¹å¾´é‡
struct AudioFeatures {
    let pitch: Float
    let volume: Float
    let speakingRate: Float
    let spectralCentroid: Float
    let zeroCrossingRate: Float
    let mfcc: [Float] // Mel-frequency cepstral coefficients
}

// MARK: - éŸ³å£°æ„Ÿæƒ…åˆ†æå®Ÿè£…
final class VoiceEmotionAnalyzer: VoiceEmotionAnalyzerProtocol {
    
    // MARK: - ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£
    private let audioEngine = AVAudioEngine()
    private let audioSession = AVAudioSession.sharedInstance()
    
    // MARK: - éŸ³å£°æ„Ÿæƒ…åˆ†æ
    func analyzeEmotion(from audioData: Data) async throws -> VoiceEmotionResult {
        // éŸ³å£°ç‰¹å¾´é‡ã®æŠ½å‡º
        let features = try await extractAudioFeatures(from: audioData)
        
        // æ„Ÿæƒ…åˆ†æã®å®Ÿè¡Œ
        let emotion = try await classifyEmotion(from: features)
        
        // çµæœã®ä½œæˆ
        return VoiceEmotionResult(
            emotion: emotion.type,
            confidence: emotion.confidence,
            arousal: features.volume, // éŸ³é‡ã‚’èˆˆå¥®åº¦ã®æŒ‡æ¨™ã¨ã—ã¦ä½¿ç”¨
            valence: features.pitch / 1000.0, // ãƒ”ãƒƒãƒã‚’ä¾¡å€¤åˆ¤æ–­ã®æŒ‡æ¨™ã¨ã—ã¦ä½¿ç”¨
            pitch: features.pitch,
            volume: features.volume,
            speakingRate: features.speakingRate,
            timestamp: Date()
        )
    }
    
    // MARK: - éŸ³å£°ç‰¹å¾´é‡ã®æŠ½å‡º
    func extractAudioFeatures(from audioData: Data) async throws -> AudioFeatures {
        // éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’AVAudioPCMBufferã«å¤‰æ›
        guard let audioBuffer = try? createAudioBuffer(from: audioData) else {
            throw VoiceEmotionAnalysisError.audioDataInvalid
        }
        
        // ãƒ”ãƒƒãƒã®è¨ˆç®—
        let pitch = try await calculatePitch(from: audioBuffer)
        
        // éŸ³é‡ã®è¨ˆç®—
        let volume = try await calculateVolume(from: audioBuffer)
        
        // è©±é€Ÿã®è¨ˆç®—
        let speakingRate = try await calculateSpeakingRate(from: audioBuffer)
        
        // ã‚¹ãƒšã‚¯ãƒˆãƒ©ãƒ«é‡å¿ƒã®è¨ˆç®—
        let spectralCentroid = try await calculateSpectralCentroid(from: audioBuffer)
        
        // ã‚¼ãƒ­ã‚¯ãƒ­ãƒƒã‚·ãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆã®è¨ˆç®—
        let zeroCrossingRate = try await calculateZeroCrossingRate(from: audioBuffer)
        
        // MFCCã®è¨ˆç®—
        let mfcc = try await calculateMFCC(from: audioBuffer)
        
        return AudioFeatures(
            pitch: pitch,
            volume: volume,
            speakingRate: speakingRate,
            spectralCentroid: spectralCentroid,
            zeroCrossingRate: zeroCrossingRate,
            mfcc: mfcc
        )
    }
    
    // MARK: - éŸ³å£°ãƒãƒƒãƒ•ã‚¡ã®ä½œæˆ
    private func createAudioBuffer(from audioData: Data) throws -> AVAudioPCMBuffer {
        let audioFormat = AVAudioFormat(standardFormatWithSampleRate: 44100, channels: 1)!
        let frameCount = UInt32(audioData.count / MemoryLayout<Float>.size)
        
        guard let audioBuffer = AVAudioPCMBuffer(pcmFormat: audioFormat, frameCapacity: frameCount) else {
            throw VoiceEmotionAnalysisError.audioDataInvalid
        }
        
        audioBuffer.frameLength = frameCount
        
        // éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’ãƒãƒƒãƒ•ã‚¡ã«ã‚³ãƒ”ãƒ¼
        let audioSamples = audioData.withUnsafeBytes { bytes in
            Array(bytes.bindMemory(to: Float.self))
        }
        
        for i in 0..<Int(frameCount) {
            audioBuffer.floatChannelData?[0][i] = audioSamples[i]
        }
        
        return audioBuffer
    }
    
    // MARK: - ãƒ”ãƒƒãƒã®è¨ˆç®—
    private func calculatePitch(from audioBuffer: AVAudioPCMBuffer) async throws -> Float {
        // ç°¡æ˜“çš„ãªãƒ”ãƒƒãƒè¨ˆç®—ï¼ˆå®Ÿéš›ã®å®Ÿè£…ã§ã¯ã‚ˆã‚Šé«˜åº¦ãªã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ä½¿ç”¨ï¼‰
        guard let channelData = audioBuffer.floatChannelData?[0] else {
            throw VoiceEmotionAnalysisError.featureExtractionFailed
        }
        
        let frameCount = Int(audioBuffer.frameLength)
        var sum: Float = 0
        var count = 0
        
        for i in 0..<frameCount {
            let sample = channelData[i]
            if abs(sample) > 0.01 { // ãƒã‚¤ã‚ºãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                sum += abs(sample)
                count += 1
            }
        }
        
        let averageAmplitude = count > 0 ? sum / Float(count) : 0
        // æŒ¯å¹…ã‹ã‚‰ãƒ”ãƒƒãƒã‚’æ¨å®šï¼ˆç°¡æ˜“çš„ãªå®Ÿè£…ï¼‰
        return averageAmplitude * 1000.0
    }
    
    // MARK: - éŸ³é‡ã®è¨ˆç®—
    private func calculateVolume(from audioBuffer: AVAudioPCMBuffer) async throws -> Float {
        guard let channelData = audioBuffer.floatChannelData?[0] else {
            throw VoiceEmotionAnalysisError.featureExtractionFailed
        }
        
        let frameCount = Int(audioBuffer.frameLength)
        var sum: Float = 0
        
        for i in 0..<frameCount {
            sum += channelData[i] * channelData[i]
        }
        
        let rms = sqrt(sum / Float(frameCount))
        // RMSã‚’dBã«å¤‰æ›
        return 20 * log10(max(rms, 0.0001))
    }
    
    // MARK: - è©±é€Ÿã®è¨ˆç®—
    private func calculateSpeakingRate(from audioBuffer: AVAudioPCMBuffer) async throws -> Float {
        // ç°¡æ˜“çš„ãªè©±é€Ÿè¨ˆç®—ï¼ˆå®Ÿéš›ã®å®Ÿè£…ã§ã¯éŸ³å£°èªè­˜çµæœã‚’ä½¿ç”¨ï¼‰
        let duration = Double(audioBuffer.frameLength) / audioBuffer.format.sampleRate
        // ä»®ã®è©±é€Ÿï¼ˆå®Ÿéš›ã®å®Ÿè£…ã§ã¯éŸ³å£°èªè­˜çµæœã‹ã‚‰è¨ˆç®—ï¼‰
        return Float(150.0 / duration) // 150 words per minute ã‚’åŸºæº–
    }
    
    // MARK: - ã‚¹ãƒšã‚¯ãƒˆãƒ©ãƒ«é‡å¿ƒã®è¨ˆç®—
    private func calculateSpectralCentroid(from audioBuffer: AVAudioPCMBuffer) async throws -> Float {
        // ç°¡æ˜“çš„ãªã‚¹ãƒšã‚¯ãƒˆãƒ©ãƒ«é‡å¿ƒè¨ˆç®—
        guard let channelData = audioBuffer.floatChannelData?[0] else {
            throw VoiceEmotionAnalysisError.featureExtractionFailed
        }
        
        let frameCount = Int(audioBuffer.frameLength)
        var sum: Float = 0
        var weightSum: Float = 0
        
        for i in 0..<frameCount {
            let sample = channelData[i]
            let weight = abs(sample)
            sum += Float(i) * weight
            weightSum += weight
        }
        
        return weightSum > 0 ? sum / weightSum : 0
    }
    
    // MARK: - ã‚¼ãƒ­ã‚¯ãƒ­ãƒƒã‚·ãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆã®è¨ˆç®—
    private func calculateZeroCrossingRate(from audioBuffer: AVAudioPCMBuffer) async throws -> Float {
        guard let channelData = audioBuffer.floatChannelData?[0] else {
            throw VoiceEmotionAnalysisError.featureExtractionFailed
        }
        
        let frameCount = Int(audioBuffer.frameLength)
        var crossings = 0
        
        for i in 1..<frameCount {
            if (channelData[i] >= 0) != (channelData[i-1] >= 0) {
                crossings += 1
            }
        }
        
        return Float(crossings) / Float(frameCount - 1)
    }
    
    // MARK: - MFCCã®è¨ˆç®—
    private func calculateMFCC(from audioBuffer: AVAudioPCMBuffer) async throws -> [Float] {
        // ç°¡æ˜“çš„ãªMFCCè¨ˆç®—ï¼ˆå®Ÿéš›ã®å®Ÿè£…ã§ã¯ã‚ˆã‚Šé«˜åº¦ãªã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ä½¿ç”¨ï¼‰
        guard let channelData = audioBuffer.floatChannelData?[0] else {
            throw VoiceEmotionAnalysisError.featureExtractionFailed
        }
        
        let frameCount = Int(audioBuffer.frameLength)
        let mfccCount = 13 // ä¸€èˆ¬çš„ãªMFCCã®æ•°
        
        var mfcc: [Float] = []
        
        for i in 0..<mfccCount {
            var sum: Float = 0
            let start = i * frameCount / mfccCount
            let end = min((i + 1) * frameCount / mfccCount, frameCount)
            
            for j in start..<end {
                sum += channelData[j] * channelData[j]
            }
            
            mfcc.append(sum / Float(end - start))
        }
        
        return mfcc
    }
    
    // MARK: - æ„Ÿæƒ…åˆ†é¡
    private func classifyEmotion(from features: AudioFeatures) async throws -> (type: EmotionType, confidence: Float) {
        // ç°¡æ˜“çš„ãªæ„Ÿæƒ…åˆ†é¡ï¼ˆå®Ÿéš›ã®å®Ÿè£…ã§ã¯æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ï¼‰
        
        // éŸ³é‡ã¨ãƒ”ãƒƒãƒã«åŸºã¥ãæ„Ÿæƒ…åˆ†é¡
        let volumeScore = features.volume
        let pitchScore = features.pitch / 1000.0
        
        // æ„Ÿæƒ…ã®åˆ¤å®š
        if volumeScore > 0.5 && pitchScore > 0.5 {
            return (.excited, 0.8)
        } else if volumeScore < -0.5 && pitchScore < 0.3 {
            return (.calm, 0.7)
        } else if volumeScore > 0.3 && pitchScore < 0.3 {
            return (.stressed, 0.6)
        } else if pitchScore > 0.4 {
            return (.positive, 0.7)
        } else if pitchScore < 0.2 {
            return (.negative, 0.6)
        } else {
            return (.neutral, 0.5)
        }
    }
}

// MARK: - éŸ³å£°æ„Ÿæƒ…åˆ†æã®æ‹¡å¼µ
extension VoiceEmotionAnalyzer {
    
    // MARK: - æ„Ÿæƒ…ã®çµ±åˆåˆ†æ
    func analyzeCombinedEmotion(textResult: TextEmotionResult, voiceResult: VoiceEmotionResult) -> CombinedEmotionResult {
        // ãƒ†ã‚­ã‚¹ãƒˆåˆ†æã¨éŸ³å£°åˆ†æã®çµæœã‚’çµ±åˆ
        let combinedConfidence = (textResult.confidence + voiceResult.confidence) / 2.0
        
        // æ„Ÿæƒ…ã®çµ±åˆï¼ˆç°¡æ˜“çš„ãªå®Ÿè£…ï¼‰
        let combinedEmotion: EmotionType
        if textResult.emotion == voiceResult.emotion {
            combinedEmotion = textResult.emotion
        } else {
            // ã‚ˆã‚Šé«˜ã„ä¿¡é ¼åº¦ã®çµæœã‚’æ¡ç”¨
            combinedEmotion = textResult.confidence > voiceResult.confidence ? textResult.emotion : voiceResult.emotion
        }
        
        return CombinedEmotionResult(
            emotion: combinedEmotion,
            confidence: combinedConfidence,
            textConfidence: textResult.confidence,
            voiceConfidence: voiceResult.confidence,
            arousal: voiceResult.arousal,
            valence: voiceResult.valence,
            timestamp: Date()
        )
    }
}

// MARK: - çµ±åˆæ„Ÿæƒ…åˆ†æçµæœ
struct CombinedEmotionResult {
    let emotion: EmotionType
    let confidence: Float
    let textConfidence: Float
    let voiceConfidence: Float
    let arousal: Float
    let valence: Float
    let timestamp: Date
}
