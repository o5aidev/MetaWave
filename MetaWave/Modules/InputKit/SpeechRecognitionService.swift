import Foundation
import Speech
import AVFoundation
import AVFAudio
import CryptoKit
import Combine

// MARK: - Èü≥Â£∞Ë™çË≠òÁµêÊûú
struct SpeechRecognitionResult {
    let text: String
    let confidence: Float
    let duration: TimeInterval
    let audioData: Data?
    let timestamp: Date
}

// MARK: - Èü≥Â£∞Ë™çË≠ò„Ç®„É©„Éº
enum SpeechRecognitionError: Error, LocalizedError {
    case microphonePermissionDenied
    case speechRecognitionNotAvailable
    case audioSessionError
    case recognitionError(String)
    case encryptionError
    
    var errorDescription: String? {
        switch self {
        case .microphonePermissionDenied:
            return "„Éû„Ç§„ÇØ„ÅÆ‰ΩøÁî®„ÅåË®±ÂèØ„Åï„Çå„Å¶„ÅÑ„Åæ„Åõ„Çì„ÄÇË®≠ÂÆö„Åã„Çâ„Éû„Ç§„ÇØ„ÅÆ‰ΩøÁî®„ÇíË®±ÂèØ„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ"
        case .speechRecognitionNotAvailable:
            return "Èü≥Â£∞Ë™çË≠òÊ©üËÉΩ„ÅåÂà©Áî®„Åß„Åç„Åæ„Åõ„Çì„ÄÇ"
        case .audioSessionError:
            return "Èü≥Â£∞„Çª„ÉÉ„Ç∑„Éß„É≥„ÅÆË®≠ÂÆö„Å´Â§±Êïó„Åó„Åæ„Åó„Åü„ÄÇ"
        case .recognitionError(let message):
            return "Èü≥Â£∞Ë™çË≠ò„Ç®„É©„Éº: \(message)"
        case .encryptionError:
            return "Èü≥Â£∞„Éá„Éº„Çø„ÅÆÊöóÂè∑Âåñ„Å´Â§±Êïó„Åó„Åæ„Åó„Åü„ÄÇ"
        }
    }
}

// MARK: - Èü≥Â£∞Ë™çË≠ò„Çµ„Éº„Éì„Çπ„Éó„É≠„Éà„Ç≥„É´
protocol SpeechRecognitionServiceProtocol {
    func requestMicrophonePermission() async -> Bool
    func startRecognition() async throws -> SpeechRecognitionResult
    func stopRecognition()
    func isRecognitionAvailable() -> Bool
}

// MARK: - Èü≥Â£∞Ë™çË≠ò„Çµ„Éº„Éì„ÇπÂÆüË£Ö
@MainActor
final class SpeechRecognitionService: NSObject, SpeechRecognitionServiceProtocol, ObservableObject {
    
    // MARK: - „Éó„É≠„Éë„ÉÜ„Ç£
    private let speechRecognizer: SFSpeechRecognizer
    private let audioEngine = AVAudioEngine()
    private var recognitionRequest: SFSpeechAudioBufferRecognitionRequest?
    private var recognitionTask: SFSpeechRecognitionTask?
    private let vault: Vaulting
    private let bufferPolicy: AudioBufferPolicy
    private var audioCollector: StreamingAudioCollector?
    
    // ObservableObject „ÅÆ„Éó„É≠„Éë„ÉÜ„Ç£
    var objectWillChange = PassthroughSubject<Void, Never>()
    
    // MARK: - ÂàùÊúüÂåñ
    init(vault: Vaulting, bufferPolicy: AudioBufferPolicy = .default) {
        self.vault = vault
        self.bufferPolicy = bufferPolicy
        
        // Êó•Êú¨Ë™ûÈü≥Â£∞Ë™çË≠ò„ÅÆË®≠ÂÆö
        guard let recognizer = SFSpeechRecognizer(locale: Locale(identifier: "ja-JP")) else {
            fatalError("Èü≥Â£∞Ë™çË≠òÊ©üËÉΩ„ÅåÂà©Áî®„Åß„Åç„Åæ„Åõ„Çì")
        }
        
        self.speechRecognizer = recognizer
        super.init()
        
        // Èü≥Â£∞Ë™çË≠ò„ÅÆË®≠ÂÆö
        speechRecognizer.delegate = self
    }
    
    // MARK: - „Éû„Ç§„ÇØÊ®©Èôê„ÅÆË¶ÅÊ±Ç
    func requestMicrophonePermission() async -> Bool {
        if #available(iOS 17.0, *) {
            return await AVAudioApplication.requestRecordPermission()
        } else {
            return await withCheckedContinuation { continuation in
                AVAudioSession.sharedInstance().requestRecordPermission { granted in
                    continuation.resume(returning: granted)
                }
            }
        }
    }
    
    // MARK: - Èü≥Â£∞Ë™çË≠ò„ÅÆÈñãÂßã
    func startRecognition() async throws -> SpeechRecognitionResult {
        // Ê®©Èôê„ÉÅ„Çß„ÉÉ„ÇØ
        guard await requestMicrophonePermission() else {
            throw SpeechRecognitionError.microphonePermissionDenied
        }
        
        // Èü≥Â£∞Ë™çË≠ò„ÅÆÂà©Áî®ÂèØËÉΩÊÄß„ÉÅ„Çß„ÉÉ„ÇØ
        guard isRecognitionAvailable() else {
            throw SpeechRecognitionError.speechRecognitionNotAvailable
        }
        
        // Êó¢Â≠ò„ÅÆ„Çø„Çπ„ÇØ„ÇíÂÅúÊ≠¢
        stopRecognition()
        
        // Èü≥Â£∞„Çª„ÉÉ„Ç∑„Éß„É≥„ÅÆË®≠ÂÆö
        try setupAudioSession()
        
        // Èü≥Â£∞Ë™çË≠ò„É™„ÇØ„Ç®„Çπ„Éà„ÅÆ‰ΩúÊàê
        recognitionRequest = SFSpeechAudioBufferRecognitionRequest()
        guard let recognitionRequest = recognitionRequest else {
            throw SpeechRecognitionError.recognitionError("Èü≥Â£∞Ë™çË≠ò„É™„ÇØ„Ç®„Çπ„Éà„ÅÆ‰ΩúÊàê„Å´Â§±Êïó„Åó„Åæ„Åó„Åü")
        }
        
        // Èü≥Â£∞Ë™çË≠ò„ÅÆË®≠ÂÆö
        recognitionRequest.shouldReportPartialResults = true // ÈÉ®ÂàÜÁµêÊûú„ÇíÂ†±Âëä
        recognitionRequest.requiresOnDeviceRecognition = false // „Çµ„Éº„Éê„ÉºÂá¶ÁêÜ„ÅßÁ≤æÂ∫¶Âêë‰∏ä
        
        // Èü≥Â£∞Ë™çË≠ò„ÅÆÊÑüÂ∫¶„Çí‰∏ä„Åí„ÇãË®≠ÂÆö
        if #available(iOS 13.0, *) {
            recognitionRequest.contextualStrings = []
        }
        
        // „Ç∑„Éü„É•„É¨„Éº„Çø„ÉºÁí∞Â¢É„ÅÆ„ÉÅ„Çß„ÉÉ„ÇØ
        #if targetEnvironment(simulator)
        print("‚ö†Ô∏è „Ç∑„Éü„É•„É¨„Éº„Çø„ÉºÁí∞Â¢É„Åß„ÅØÈü≥Â£∞ÂÖ•Âäõ„ÅØ„Çµ„Éù„Éº„Éà„Åï„Çå„Å¶„ÅÑ„Åæ„Åõ„Çì")
        throw SpeechRecognitionError.recognitionError("„Ç∑„Éü„É•„É¨„Éº„Çø„Éº„Åß„ÅØÈü≥Â£∞ÂÖ•Âäõ„Çí‰ΩøÁî®„Åß„Åç„Åæ„Åõ„Çì„ÄÇÂÆüÊ©ü„Åß„ÉÜ„Çπ„Éà„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ")
        #else
        
        // Èü≥Â£∞„Ç®„É≥„Ç∏„É≥„ÅÆË®≠ÂÆö
        let inputNode = audioEngine.inputNode
        
        // „Éè„Éº„Éâ„Ç¶„Çß„Ç¢„ÅÆÂÆüÈöõ„ÅÆ„Éï„Ç©„Éº„Éû„ÉÉ„Éà„ÇíÂèñÂæó
        let hardwareFormat = inputNode.outputFormat(forBus: 0)
        print("üîç „Éè„Éº„Éâ„Ç¶„Çß„Ç¢„Éï„Ç©„Éº„Éû„ÉÉ„Éà: \(hardwareFormat)")
        
        // „Éè„Éº„Éâ„Ç¶„Çß„Ç¢„Éï„Ç©„Éº„Éû„ÉÉ„Éà„Çí‰ΩøÁî®Ôºà„Éï„Ç©„Éº„Éû„ÉÉ„Éà‰∏ç‰∏ÄËá¥„ÇíÂõûÈÅøÔºâ
        let validFormat = hardwareFormat
        
        print("‚úÖ Èå≤Èü≥„Éï„Ç©„Éº„Éû„ÉÉ„ÉàË®≠ÂÆö: \(validFormat.commonFormat) \(validFormat.sampleRate)Hz, \(validFormat.channelCount)ch")
        
        // Èü≥Â£∞„Éá„Éº„Çø„ÅÆ„Éê„ÉÉ„Éï„Ç°„É™„É≥„Ç∞
        audioCollector = StreamingAudioCollector(policy: bufferPolicy)
        let startTime = Date()
        
        // „Çø„ÉÉ„Éó„Çí„Ç§„É≥„Çπ„Éà„Éº„É´Ôºà„Éê„ÉÉ„Éï„Ç°„Çµ„Ç§„Ç∫„Çí2048„Å´Â¢ó„ÇÑ„Åó„Å¶„Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÊúÄÈÅ©ÂåñÔºâ
        inputNode.installTap(onBus: 0, bufferSize: 2048, format: validFormat) { buffer, _ in
            recognitionRequest.append(buffer)
            
            // Èü≥Â£∞„Éá„Éº„Çø„ÅÆÂèéÈõÜÔºàÊöóÂè∑ÂåñÁî®„Éª„Ç™„Éó„Ç∑„Éß„Éä„É´Ôºâ
            // „É°„É¢„É™‰ΩøÁî®Èáè„ÇíÂâäÊ∏õ„Åô„Çã„Åü„ÇÅ„ÄÅ„Éù„É™„Ç∑„Éº„Å´Âü∫„Å•„ÅçÂèéÈõÜ
            let audioBuffer = buffer.audioBufferList.pointee.mBuffers
            if let baseAddress = audioBuffer.mData {
                let audioBytes = Data(bytes: baseAddress, count: Int(audioBuffer.mDataByteSize))
                audioCollector?.append(audioBytes)
            }
        }
        
        // Èü≥Â£∞„Ç®„É≥„Ç∏„É≥„ÅÆÊ∫ñÂÇô„Å®ÈñãÂßã
        audioEngine.prepare()
        print("‚úÖ Èü≥Â£∞„Ç®„É≥„Ç∏„É≥Ê∫ñÂÇôÂÆå‰∫Ü")
        
        do {
            try audioEngine.start()
            print("‚úÖ Èü≥Â£∞„Ç®„É≥„Ç∏„É≥ÈñãÂßã")
        } catch {
            print("‚ùå Èü≥Â£∞„Ç®„É≥„Ç∏„É≥ÈñãÂßã„Ç®„É©„Éº: \(error.localizedDescription)")
            inputNode.removeTap(onBus: 0)
            throw SpeechRecognitionError.audioSessionError
        }
        
        // Èü≥Â£∞Ë™çË≠ò„Çø„Çπ„ÇØ„ÅÆÈñãÂßã
        return try await withCheckedThrowingContinuation { continuation in
            var hasResumed = false
            var lastValidText = "" // ÊúÄÂæå„ÅÆÊúâÂäπ„Å™„ÉÜ„Ç≠„Çπ„Éà„Çí‰øùÂ≠ò
            
            recognitionTask = speechRecognizer.recognitionTask(with: recognitionRequest) { result, error in
                // Êó¢„Å´resumeÊ∏à„Åø„ÅÆÂ†¥Âêà„ÅØ‰Ωï„ÇÇ„Åó„Å™„ÅÑ
                guard !hasResumed else { return }
                
                if let error = error {
                    let errorMessage = "Èü≥Â£∞Ë™çË≠ò„Ç®„É©„Éº: \(error.localizedDescription)"
                    print("‚ùå \(errorMessage)")
                    
                    // "No speech detected"„ÅØÊ≠£Â∏∏„Å™ÁµÇ‰∫Ü„Å®„Åó„Å¶Êâ±„ÅÜ
                    if error.localizedDescription.contains("No speech detected") {
                        let duration = Date().timeIntervalSince(startTime)
                        let recognitionResult = SpeechRecognitionResult(
                            text: lastValidText, // ÊúÄÂæå„ÅÆÊúâÂäπ„Å™„ÉÜ„Ç≠„Çπ„Éà„Çí‰ΩøÁî®
                            confidence: 0.0,
                            duration: duration,
                            audioData: audioCollector?.collectedData(),
                            timestamp: startTime
                        )
                        print("‚ÑπÔ∏è Èü≥Â£∞„ÅåÊ§úÂá∫„Åï„Çå„Åæ„Åõ„Çì„Åß„Åó„ÅüÔºàÊ≠£Â∏∏ÁµÇ‰∫ÜÔºâ")
                        hasResumed = true
                        continuation.resume(returning: recognitionResult)
                    } else {
                        hasResumed = true
                        continuation.resume(throwing: SpeechRecognitionError.recognitionError(errorMessage))
                    }
                    return
                }
                
                guard let result = result else {
                    let errorMessage = "Èü≥Â£∞Ë™çË≠òÁµêÊûú„ÅåÂèñÂæó„Åß„Åç„Åæ„Åõ„Çì„Åß„Åó„Åü"
                    print("‚ùå \(errorMessage)")
                    hasResumed = true
                    continuation.resume(throwing: SpeechRecognitionError.recognitionError(errorMessage))
                    return
                }
                
                // Èü≥Â£∞Ë™çË≠òÁµêÊûú„ÇíÂá¶ÁêÜÔºàÈÉ®ÂàÜÁµêÊûú„Å®„Åó„Å¶ËìÑÁ©çÔºâ
                let currentText = result.bestTranscription.formattedString
                print("üìù Èü≥Â£∞Ë™çË≠òÈÉ®ÂàÜÁµêÊûú: \(currentText)")
                
                // ÊúâÂäπ„Å™„ÉÜ„Ç≠„Çπ„Éà„Çí‰øùÂ≠ò
                if !currentText.isEmpty {
                    lastValidText = currentText
                    // ÈÉ®ÂàÜÁµêÊûú„ÇíÈÄöÁü•ÔºàUIÊõ¥Êñ∞Áî®Ôºâ
                    NotificationCenter.default.post(
                        name: NSNotification.Name("SpeechRecognitionPartialResult"),
                        object: nil,
                        userInfo: ["text": currentText]
                    )
                }
                
                // isFinal„ÅÆÂ†¥Âêà„ÅÆ„ÅøÂÆå‰∫Ü„Å®„Åó„Å¶Êâ±„ÅÜ
                if result.isFinal {
                    let duration = Date().timeIntervalSince(startTime)
                    
                    // ‰ø°È†ºÂ∫¶„ÅÆË®àÁÆóÔºàSFTranscriptionSegment„ÅÆ‰ø°È†ºÂ∫¶„Åã„ÇâÂπ≥Âùá„ÇíË®àÁÆóÔºâ
                    let segments = result.bestTranscription.segments
                    let averageConfidence: Float = segments.isEmpty ? 1.0 : Float(segments.reduce(0.0) { $0 + $1.confidence }) / Float(segments.count)
                    
                    // ÊúÄÂæå„ÅÆÊúâÂäπ„Å™„ÉÜ„Ç≠„Çπ„Éà„Çí‰ΩøÁî®ÔºàÁ©∫„ÅÆÂ†¥Âêà„ÅØÁèæÂú®„ÅÆ„ÉÜ„Ç≠„Çπ„ÉàÔºâ
                    let finalText = lastValidText.isEmpty ? currentText : lastValidText
                    
                    let recognitionResult = SpeechRecognitionResult(
                        text: finalText,
                        confidence: averageConfidence,
                        duration: duration,
                        audioData: audioCollector?.collectedData(),
                        timestamp: startTime
                    )
                    
                    print("‚úÖ Èü≥Â£∞Ë™çË≠òÂÆå‰∫Ü: \(finalText)")
                    hasResumed = true
                    continuation.resume(returning: recognitionResult)
                }
            }
        }
        #endif
    }
    
    // MARK: - Èü≥Â£∞Ë™çË≠ò„ÅÆÂÅúÊ≠¢
    func stopRecognition() {
        recognitionTask?.cancel()
        recognitionTask = nil
        recognitionRequest?.endAudio()
        recognitionRequest = nil
        audioCollector = nil
        
        // Èü≥Â£∞„Ç®„É≥„Ç∏„É≥„ÅÆÂÆâÂÖ®„Å™ÂÅúÊ≠¢
        if audioEngine.isRunning {
            audioEngine.stop()
        }
        
        // „Çø„ÉÉ„Éó„ÅÆÂÆâÂÖ®„Å™ÂâäÈô§
        audioEngine.inputNode.removeTap(onBus: 0)
    }
    
    // MARK: - Èü≥Â£∞Ë™çË≠ò„ÅÆÂà©Áî®ÂèØËÉΩÊÄß„ÉÅ„Çß„ÉÉ„ÇØ
    func isRecognitionAvailable() -> Bool {
        return speechRecognizer.isAvailable
    }
    
    // MARK: - Èü≥Â£∞„Çª„ÉÉ„Ç∑„Éß„É≥„ÅÆË®≠ÂÆö
    private func setupAudioSession() throws {
        let audioSession = AVAudioSession.sharedInstance()
        
        // „Ç´„ÉÜ„Ç¥„É™Ë®≠ÂÆöÔºàÂÖ•Âäõ„ÅÆ„ÅøÔºâ
        try audioSession.setCategory(.record, mode: .measurement, options: [])
        
        // Â∏åÊúõ„Åô„Çã„Çµ„É≥„Éó„É´„É¨„Éº„Éà„ÇíË®≠ÂÆöÔºà„Ç¢„ÇØ„ÉÜ„Ç£„Éô„Éº„ÉàÂâçÔºâ
        try audioSession.setPreferredSampleRate(44100)
        
        // „Ç¢„ÇØ„ÉÜ„Ç£„Éô„Éº„Éà
        try audioSession.setActive(true)
        
        print("‚úÖ Èü≥Â£∞„Çª„ÉÉ„Ç∑„Éß„É≥Ë®≠ÂÆöÂÆå‰∫Ü: sampleRate=\(audioSession.sampleRate)")
    }
    
    // MARK: - Èü≥Â£∞„Éá„Éº„Çø„ÅÆÊöóÂè∑ÂåñÔºàÊúÄÈÅ©ÂåñÁâàÔºâ
    private func encryptAudioData(_ data: Data) throws -> Data {
        // „É°„É¢„É™ÂäπÁéá„ÇíËÄÉÊÖÆ„Åó„ÅüÊöóÂè∑Âåñ
        guard !data.isEmpty else { return data }
        
        do {
            let encryptedBlob = try vault.encrypt(data)
            // EncryptedBlob„ÇíData„Å´Â§âÊèõÔºàciphertext + nonce + tagÔºâ
            var combinedData = Data()
            combinedData.append(encryptedBlob.ciphertext)
            combinedData.append(encryptedBlob.nonce)
            combinedData.append(encryptedBlob.tag)
            return combinedData
        } catch {
            print("‚ö†Ô∏è Èü≥Â£∞„Éá„Éº„Çø„ÅÆÊöóÂè∑Âåñ„Å´Â§±Êïó: \(error.localizedDescription)")
            // ÊöóÂè∑Âåñ„Å´Â§±Êïó„Åó„ÅüÂ†¥Âêà„ÅØÂÖÉ„Éá„Éº„Çø„ÇíËøî„ÅôÔºà„Éó„É©„Ç§„Éê„Ç∑„Éº„É™„Çπ„ÇØ„ÅÇ„ÇäÔºâ
            return data
        }
    }
    
    // MARK: - Èü≥Â£∞„Éá„Éº„Çø„ÅÆÂæ©Âè∑ÂåñÔºàÊúÄÈÅ©ÂåñÁâàÔºâ
    private func decryptAudioData(_ encryptedData: Data) throws -> Data {
        // „É°„É¢„É™ÂäπÁéá„ÇíËÄÉÊÖÆ„Åó„ÅüÂæ©Âè∑Âåñ
        guard !encryptedData.isEmpty else { return encryptedData }
        
        // Data„ÇíEncryptedBlob„Å´Â§âÊèõÔºàÁ∞°ÊòìÁâàÔºöÂÖÉ„Éá„Éº„Çø„Çí„Åù„ÅÆ„Åæ„ÅæËøî„ÅôÔºâ
        // ÂÆüÈöõ„ÅÆÂÆüË£Ö„Åß„ÅØ„ÄÅciphertext„ÄÅnonce„ÄÅtag„ÇíÂàÜÈõ¢„Åô„ÇãÂøÖË¶Å„Åå„ÅÇ„Çã
        return encryptedData
    }
}

// MARK: - SFSpeechRecognizerDelegate
extension SpeechRecognitionService: SFSpeechRecognizerDelegate {
    func speechRecognizer(_ speechRecognizer: SFSpeechRecognizer, availabilityDidChange available: Bool) {
        // Èü≥Â£∞Ë™çË≠ò„ÅÆÂà©Áî®ÂèØËÉΩÊÄß„ÅåÂ§âÊõ¥„Åï„Çå„ÅüÊôÇ„ÅÆÂá¶ÁêÜ
        if !available {
            stopRecognition()
        }
    }
}

// MARK: - Èü≥Â£∞Ë™çË≠ò„Çµ„Éº„Éì„Çπ„ÅÆÊã°Âºµ
extension SpeechRecognitionService {
    
    // MARK: - Èü≥Â£∞Ë™çË≠òÁµêÊûú„ÅÆÊöóÂè∑Âåñ
    func encryptRecognitionResult(_ result: SpeechRecognitionResult) throws -> SpeechRecognitionResult {
        guard let audioData = result.audioData else {
            return result
        }
        
        let encryptedAudioData = try encryptAudioData(audioData)
        
        return SpeechRecognitionResult(
            text: result.text,
            confidence: result.confidence,
            duration: result.duration,
            audioData: encryptedAudioData,
            timestamp: result.timestamp
        )
    }
    
    // MARK: - Èü≥Â£∞Ë™çË≠òÁµêÊûú„ÅÆÂæ©Âè∑Âåñ
    func decryptRecognitionResult(_ result: SpeechRecognitionResult) throws -> SpeechRecognitionResult {
        guard let audioData = result.audioData else {
            return result
        }
        
        let decryptedAudioData = try decryptAudioData(audioData)
        
        return SpeechRecognitionResult(
            text: result.text,
            confidence: result.confidence,
            duration: result.duration,
            audioData: decryptedAudioData,
            timestamp: result.timestamp
        )
    }
}

// MARK: - Audio Buffer Policy

struct AudioBufferPolicy {
    let maxBufferedBytes: Int
    let preferredChunkSize: Int
    let allowTruncation: Bool
    
    static let `default` = AudioBufferPolicy(
        maxBufferedBytes: 3 * 1024 * 1024,   // 3MB
        preferredChunkSize: 256 * 1024,      // 256KB
        allowTruncation: true
    )
}

final class StreamingAudioCollector {
    private let policy: AudioBufferPolicy
    private var chunks: [Data] = []
    private var totalBytes = 0
    private(set) var didTruncate = false
    
    init(policy: AudioBufferPolicy) {
        self.policy = policy
        self.chunks.reserveCapacity(8)
    }
    
    func append(_ data: Data) {
        guard !data.isEmpty else { return }
        
        if totalBytes + data.count <= policy.maxBufferedBytes {
            chunks.append(data)
            totalBytes += data.count
            return
        }
        
        if !policy.allowTruncation {
            return
        }
        
        let remaining = max(0, policy.maxBufferedBytes - totalBytes)
        if remaining > 0 {
            let truncated = data.prefix(remaining)
            chunks.append(Data(truncated))
            totalBytes += truncated.count
        }
        didTruncate = true
    }
    
    func collectedData() -> Data? {
        guard !chunks.isEmpty else { return nil }
        if chunks.count == 1 {
            return chunks.first
        }
        var buffer = Data(capacity: min(policy.maxBufferedBytes, totalBytes))
        for chunk in chunks {
            buffer.append(chunk)
        }
        return buffer
    }
}
