import Foundation
import Speech
import AVFoundation
import AVFAudio
import CryptoKit
import Combine

// MARK: - éŸ³å£°èªè­˜çµæœ
struct SpeechRecognitionResult {
    let text: String
    let confidence: Float
    let duration: TimeInterval
    let audioData: Data?
    let timestamp: Date
}

// MARK: - éŸ³å£°èªè­˜ã‚¨ãƒ©ãƒ¼
enum SpeechRecognitionError: Error, LocalizedError {
    case microphonePermissionDenied
    case speechRecognitionNotAvailable
    case audioSessionError
    case recognitionError(String)
    case encryptionError
    
    var errorDescription: String? {
        switch self {
        case .microphonePermissionDenied:
            return "ãƒã‚¤ã‚¯ã®ä½¿ç”¨ãŒè¨±å¯ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚è¨­å®šã‹ã‚‰ãƒã‚¤ã‚¯ã®ä½¿ç”¨ã‚’è¨±å¯ã—ã¦ãã ã•ã„ã€‚"
        case .speechRecognitionNotAvailable:
            return "éŸ³å£°èªè­˜æ©Ÿèƒ½ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚"
        case .audioSessionError:
            return "éŸ³å£°ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®è¨­å®šã«å¤±æ•—ã—ã¾ã—ãŸã€‚"
        case .recognitionError(let message):
            return "éŸ³å£°èªè­˜ã‚¨ãƒ©ãƒ¼: \(message)"
        case .encryptionError:
            return "éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã®æš—å·åŒ–ã«å¤±æ•—ã—ã¾ã—ãŸã€‚"
        }
    }
}

// MARK: - éŸ³å£°èªè­˜ã‚µãƒ¼ãƒ“ã‚¹ãƒ—ãƒ­ãƒˆã‚³ãƒ«
protocol SpeechRecognitionServiceProtocol {
    func requestMicrophonePermission() async -> Bool
    func startRecognition() async throws -> SpeechRecognitionResult
    func stopRecognition()
    func isRecognitionAvailable() -> Bool
}

// MARK: - éŸ³å£°èªè­˜ã‚µãƒ¼ãƒ“ã‚¹å®Ÿè£…
@MainActor
final class SpeechRecognitionService: NSObject, SpeechRecognitionServiceProtocol, ObservableObject {
    
    // MARK: - ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£
    private let speechRecognizer: SFSpeechRecognizer
    private let audioEngine = AVAudioEngine()
    private var recognitionRequest: SFSpeechAudioBufferRecognitionRequest?
    private var recognitionTask: SFSpeechRecognitionTask?
    private let vault: Vaulting
    
    // ObservableObject ã®ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£
    var objectWillChange = PassthroughSubject<Void, Never>()
    
    // MARK: - åˆæœŸåŒ–
    init(vault: Vaulting) {
        self.vault = vault
        
        // æ—¥æœ¬èªéŸ³å£°èªè­˜ã®è¨­å®š
        guard let recognizer = SFSpeechRecognizer(locale: Locale(identifier: "ja-JP")) else {
            fatalError("éŸ³å£°èªè­˜æ©Ÿèƒ½ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
        }
        
        self.speechRecognizer = recognizer
        super.init()
        
        // éŸ³å£°èªè­˜ã®è¨­å®š
        speechRecognizer.delegate = self
    }
    
    // MARK: - ãƒã‚¤ã‚¯æ¨©é™ã®è¦æ±‚
    func requestMicrophonePermission() async -> Bool {
        if #available(iOS 17.0, *) {
            return await AVAudioApplication.requestRecordPermission()
        } else {
            return await withCheckedContinuation { continuation in
                AVAudioSession.sharedInstance().requestRecordPermission { granted in
                    continuation.resume(returning: granted)
                }
            }
        }
    }
    
    // MARK: - éŸ³å£°èªè­˜ã®é–‹å§‹
    func startRecognition() async throws -> SpeechRecognitionResult {
        // æ¨©é™ãƒã‚§ãƒƒã‚¯
        guard await requestMicrophonePermission() else {
            throw SpeechRecognitionError.microphonePermissionDenied
        }
        
        // éŸ³å£°èªè­˜ã®åˆ©ç”¨å¯èƒ½æ€§ãƒã‚§ãƒƒã‚¯
        guard isRecognitionAvailable() else {
            throw SpeechRecognitionError.speechRecognitionNotAvailable
        }
        
        // æ—¢å­˜ã®ã‚¿ã‚¹ã‚¯ã‚’åœæ­¢
        stopRecognition()
        
        // éŸ³å£°ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®è¨­å®š
        try setupAudioSession()
        
        // éŸ³å£°èªè­˜ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®ä½œæˆ
        recognitionRequest = SFSpeechAudioBufferRecognitionRequest()
        guard let recognitionRequest = recognitionRequest else {
            throw SpeechRecognitionError.recognitionError("éŸ³å£°èªè­˜ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸ")
        }
        
        // éŸ³å£°èªè­˜ã®è¨­å®š
        recognitionRequest.shouldReportPartialResults = true // éƒ¨åˆ†çµæœã‚’å ±å‘Š
        recognitionRequest.requiresOnDeviceRecognition = false // ã‚µãƒ¼ãƒãƒ¼å‡¦ç†ã§ç²¾åº¦å‘ä¸Š
        
        // éŸ³å£°èªè­˜ã®æ„Ÿåº¦ã‚’ä¸Šã’ã‚‹è¨­å®š
        if #available(iOS 13.0, *) {
            recognitionRequest.contextualStrings = []
        }
        
        // ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ãƒ¼ç’°å¢ƒã®ãƒã‚§ãƒƒã‚¯
        #if targetEnvironment(simulator)
        print("âš ï¸ ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ãƒ¼ç’°å¢ƒã§ã¯éŸ³å£°å…¥åŠ›ã¯ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã¾ã›ã‚“")
        throw SpeechRecognitionError.recognitionError("ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ãƒ¼ã§ã¯éŸ³å£°å…¥åŠ›ã‚’ä½¿ç”¨ã§ãã¾ã›ã‚“ã€‚å®Ÿæ©Ÿã§ãƒ†ã‚¹ãƒˆã—ã¦ãã ã•ã„ã€‚")
        #endif
        
        // éŸ³å£°ã‚¨ãƒ³ã‚¸ãƒ³ã®è¨­å®š
        let inputNode = audioEngine.inputNode
        
        // ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã®å®Ÿéš›ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’å–å¾—
        let hardwareFormat = inputNode.outputFormat(forBus: 0)
        print("ğŸ” ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ: \(hardwareFormat)")
        
        // ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’ä½¿ç”¨ï¼ˆãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆä¸ä¸€è‡´ã‚’å›é¿ï¼‰
        let validFormat = hardwareFormat
        
        print("âœ… éŒ²éŸ³ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆè¨­å®š: \(validFormat.commonFormat) \(validFormat.sampleRate)Hz, \(validFormat.channelCount)ch")
        
        // éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã®ãƒãƒƒãƒ•ã‚¡ãƒªãƒ³ã‚°
        var audioData = Data()
        let startTime = Date()
        
        // ã‚¿ãƒƒãƒ—ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆãƒãƒƒãƒ•ã‚¡ã‚µã‚¤ã‚ºã‚’2048ã«å¢—ã‚„ã—ã¦ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ï¼‰
        inputNode.installTap(onBus: 0, bufferSize: 2048, format: validFormat) { buffer, _ in
            recognitionRequest.append(buffer)
            
            // éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã®åé›†ï¼ˆæš—å·åŒ–ç”¨ãƒ»ã‚ªãƒ—ã‚·ãƒ§ãƒŠãƒ«ï¼‰
            // ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’å‰Šæ¸›ã™ã‚‹ãŸã‚ã€å¿…è¦ã«å¿œã˜ã¦ã®ã¿åé›†
            if audioData.count < 10 * 1024 * 1024 { // 10MBã¾ã§åˆ¶é™
                let audioBuffer = buffer.audioBufferList.pointee.mBuffers
                let audioBytes = Data(bytes: audioBuffer.mData!, count: Int(audioBuffer.mDataByteSize))
                audioData.append(audioBytes)
            }
        }
        
        // éŸ³å£°ã‚¨ãƒ³ã‚¸ãƒ³ã®æº–å‚™ã¨é–‹å§‹
        audioEngine.prepare()
        print("âœ… éŸ³å£°ã‚¨ãƒ³ã‚¸ãƒ³æº–å‚™å®Œäº†")
        
        do {
            try audioEngine.start()
            print("âœ… éŸ³å£°ã‚¨ãƒ³ã‚¸ãƒ³é–‹å§‹")
        } catch {
            print("âŒ éŸ³å£°ã‚¨ãƒ³ã‚¸ãƒ³é–‹å§‹ã‚¨ãƒ©ãƒ¼: \(error.localizedDescription)")
            inputNode.removeTap(onBus: 0)
            throw SpeechRecognitionError.audioSessionError
        }
        
        // éŸ³å£°èªè­˜ã‚¿ã‚¹ã‚¯ã®é–‹å§‹
        return try await withCheckedThrowingContinuation { continuation in
            var hasResumed = false
            var lastValidText = "" // æœ€å¾Œã®æœ‰åŠ¹ãªãƒ†ã‚­ã‚¹ãƒˆã‚’ä¿å­˜
            
            recognitionTask = speechRecognizer.recognitionTask(with: recognitionRequest) { result, error in
                // æ—¢ã«resumeæ¸ˆã¿ã®å ´åˆã¯ä½•ã‚‚ã—ãªã„
                guard !hasResumed else { return }
                
                if let error = error {
                    let errorMessage = "éŸ³å£°èªè­˜ã‚¨ãƒ©ãƒ¼: \(error.localizedDescription)"
                    print("âŒ \(errorMessage)")
                    
                    // "No speech detected"ã¯æ­£å¸¸ãªçµ‚äº†ã¨ã—ã¦æ‰±ã†
                    if error.localizedDescription.contains("No speech detected") {
                        let duration = Date().timeIntervalSince(startTime)
                        let recognitionResult = SpeechRecognitionResult(
                            text: lastValidText, // æœ€å¾Œã®æœ‰åŠ¹ãªãƒ†ã‚­ã‚¹ãƒˆã‚’ä½¿ç”¨
                            confidence: 0.0,
                            duration: duration,
                            audioData: audioData,
                            timestamp: startTime
                        )
                        print("â„¹ï¸ éŸ³å£°ãŒæ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸï¼ˆæ­£å¸¸çµ‚äº†ï¼‰")
                        hasResumed = true
                        continuation.resume(returning: recognitionResult)
                    } else {
                        hasResumed = true
                        continuation.resume(throwing: SpeechRecognitionError.recognitionError(errorMessage))
                    }
                    return
                }
                
                guard let result = result else {
                    let errorMessage = "éŸ³å£°èªè­˜çµæœãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ"
                    print("âŒ \(errorMessage)")
                    hasResumed = true
                    continuation.resume(throwing: SpeechRecognitionError.recognitionError(errorMessage))
                    return
                }
                
                // éŸ³å£°èªè­˜çµæœã‚’å‡¦ç†ï¼ˆéƒ¨åˆ†çµæœã¨ã—ã¦è“„ç©ï¼‰
                let currentText = result.bestTranscription.formattedString
                print("ğŸ“ éŸ³å£°èªè­˜éƒ¨åˆ†çµæœ: \(currentText)")
                
                // æœ‰åŠ¹ãªãƒ†ã‚­ã‚¹ãƒˆã‚’ä¿å­˜
                if !currentText.isEmpty {
                    lastValidText = currentText
                    // éƒ¨åˆ†çµæœã‚’é€šçŸ¥ï¼ˆUIæ›´æ–°ç”¨ï¼‰
                    NotificationCenter.default.post(
                        name: NSNotification.Name("SpeechRecognitionPartialResult"),
                        object: nil,
                        userInfo: ["text": currentText]
                    )
                }
                
                // isFinalã®å ´åˆã®ã¿å®Œäº†ã¨ã—ã¦æ‰±ã†
                if result.isFinal {
                    let duration = Date().timeIntervalSince(startTime)
                    
                    // ä¿¡é ¼åº¦ã®è¨ˆç®—ï¼ˆSFTranscriptionSegmentã®ä¿¡é ¼åº¦ã‹ã‚‰å¹³å‡ã‚’è¨ˆç®—ï¼‰
                    let segments = result.bestTranscription.segments
                    let averageConfidence: Float = segments.isEmpty ? 1.0 : Float(segments.reduce(0.0) { $0 + $1.confidence }) / Float(segments.count)
                    
                    // æœ€å¾Œã®æœ‰åŠ¹ãªãƒ†ã‚­ã‚¹ãƒˆã‚’ä½¿ç”¨ï¼ˆç©ºã®å ´åˆã¯ç¾åœ¨ã®ãƒ†ã‚­ã‚¹ãƒˆï¼‰
                    let finalText = lastValidText.isEmpty ? currentText : lastValidText
                    
                    let recognitionResult = SpeechRecognitionResult(
                        text: finalText,
                        confidence: averageConfidence,
                        duration: duration,
                        audioData: audioData,
                        timestamp: startTime
                    )
                    
                    print("âœ… éŸ³å£°èªè­˜å®Œäº†: \(finalText)")
                    hasResumed = true
                    continuation.resume(returning: recognitionResult)
                }
            }
        }
    }
    
    // MARK: - éŸ³å£°èªè­˜ã®åœæ­¢
    func stopRecognition() {
        recognitionTask?.cancel()
        recognitionTask = nil
        recognitionRequest?.endAudio()
        recognitionRequest = nil
        
        // éŸ³å£°ã‚¨ãƒ³ã‚¸ãƒ³ã®å®‰å…¨ãªåœæ­¢
        if audioEngine.isRunning {
            audioEngine.stop()
        }
        
        // ã‚¿ãƒƒãƒ—ã®å®‰å…¨ãªå‰Šé™¤
        audioEngine.inputNode.removeTap(onBus: 0)
    }
    
    // MARK: - éŸ³å£°èªè­˜ã®åˆ©ç”¨å¯èƒ½æ€§ãƒã‚§ãƒƒã‚¯
    func isRecognitionAvailable() -> Bool {
        return speechRecognizer.isAvailable
    }
    
    // MARK: - éŸ³å£°ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®è¨­å®š
    private func setupAudioSession() throws {
        let audioSession = AVAudioSession.sharedInstance()
        
        // ã‚«ãƒ†ã‚´ãƒªè¨­å®šï¼ˆå…¥åŠ›ã®ã¿ï¼‰
        try audioSession.setCategory(.record, mode: .measurement, options: [])
        
        // å¸Œæœ›ã™ã‚‹ã‚µãƒ³ãƒ—ãƒ«ãƒ¬ãƒ¼ãƒˆã‚’è¨­å®šï¼ˆã‚¢ã‚¯ãƒ†ã‚£ãƒ™ãƒ¼ãƒˆå‰ï¼‰
        try audioSession.setPreferredSampleRate(44100)
        
        // ã‚¢ã‚¯ãƒ†ã‚£ãƒ™ãƒ¼ãƒˆ
        try audioSession.setActive(true)
        
        print("âœ… éŸ³å£°ã‚»ãƒƒã‚·ãƒ§ãƒ³è¨­å®šå®Œäº†: sampleRate=\(audioSession.sampleRate)")
    }
    
    // MARK: - éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã®æš—å·åŒ–ï¼ˆæœ€é©åŒ–ç‰ˆï¼‰
    private func encryptAudioData(_ data: Data) throws -> Data {
        // ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã‚’è€ƒæ…®ã—ãŸæš—å·åŒ–
        guard !data.isEmpty else { return data }
        
        do {
            let encryptedBlob = try vault.encrypt(data)
            // EncryptedBlobã‚’Dataã«å¤‰æ›ï¼ˆciphertext + nonce + tagï¼‰
            var combinedData = Data()
            combinedData.append(encryptedBlob.ciphertext)
            combinedData.append(encryptedBlob.nonce)
            combinedData.append(encryptedBlob.tag)
            return combinedData
        } catch {
            print("âš ï¸ éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã®æš—å·åŒ–ã«å¤±æ•—: \(error.localizedDescription)")
            // æš—å·åŒ–ã«å¤±æ•—ã—ãŸå ´åˆã¯å…ƒãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™ï¼ˆãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ãƒªã‚¹ã‚¯ã‚ã‚Šï¼‰
            return data
        }
    }
    
    // MARK: - éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã®å¾©å·åŒ–ï¼ˆæœ€é©åŒ–ç‰ˆï¼‰
    private func decryptAudioData(_ encryptedData: Data) throws -> Data {
        // ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã‚’è€ƒæ…®ã—ãŸå¾©å·åŒ–
        guard !encryptedData.isEmpty else { return encryptedData }
        
        // Dataã‚’EncryptedBlobã«å¤‰æ›ï¼ˆç°¡æ˜“ç‰ˆï¼šå…ƒãƒ‡ãƒ¼ã‚¿ã‚’ãã®ã¾ã¾è¿”ã™ï¼‰
        // å®Ÿéš›ã®å®Ÿè£…ã§ã¯ã€ciphertextã€nonceã€tagã‚’åˆ†é›¢ã™ã‚‹å¿…è¦ãŒã‚ã‚‹
        return encryptedData
    }
}

// MARK: - SFSpeechRecognizerDelegate
extension SpeechRecognitionService: SFSpeechRecognizerDelegate {
    func speechRecognizer(_ speechRecognizer: SFSpeechRecognizer, availabilityDidChange available: Bool) {
        // éŸ³å£°èªè­˜ã®åˆ©ç”¨å¯èƒ½æ€§ãŒå¤‰æ›´ã•ã‚ŒãŸæ™‚ã®å‡¦ç†
        if !available {
            stopRecognition()
        }
    }
}

// MARK: - éŸ³å£°èªè­˜ã‚µãƒ¼ãƒ“ã‚¹ã®æ‹¡å¼µ
extension SpeechRecognitionService {
    
    // MARK: - éŸ³å£°èªè­˜çµæœã®æš—å·åŒ–
    func encryptRecognitionResult(_ result: SpeechRecognitionResult) throws -> SpeechRecognitionResult {
        guard let audioData = result.audioData else {
            return result
        }
        
        let encryptedAudioData = try encryptAudioData(audioData)
        
        return SpeechRecognitionResult(
            text: result.text,
            confidence: result.confidence,
            duration: result.duration,
            audioData: encryptedAudioData,
            timestamp: result.timestamp
        )
    }
    
    // MARK: - éŸ³å£°èªè­˜çµæœã®å¾©å·åŒ–
    func decryptRecognitionResult(_ result: SpeechRecognitionResult) throws -> SpeechRecognitionResult {
        guard let audioData = result.audioData else {
            return result
        }
        
        let decryptedAudioData = try decryptAudioData(audioData)
        
        return SpeechRecognitionResult(
            text: result.text,
            confidence: result.confidence,
            duration: result.duration,
            audioData: decryptedAudioData,
            timestamp: result.timestamp
        )
    }
}
